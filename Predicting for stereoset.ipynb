{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertForNextSentencePrediction,BertForMaskedLM\n",
    "from torch.nn.functional import softmax\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "devDataFile = r\"dev.json\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## intersentence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDevFile(devDataFile,testType):\n",
    "    with open (devDataFile, encoding='utf-8') as f:\n",
    "        devData = json.load(f)\n",
    "    devProcessed = dict()\n",
    "    for record in devData['data'][testType]:\n",
    "        rId = record['id']\n",
    "        \n",
    "        devProcessed[rId] = {}\n",
    "        devProcessed[rId]['context'] = record['context']\n",
    "        devProcessed[rId]['bias_type'] = record['bias_type']\n",
    "        sentences = []\n",
    "        for row in record['sentences']:\n",
    "            tmp = dict()\n",
    "            tmp['sentence'] = row['sentence']\n",
    "            tmp['gold_label'] = row['gold_label']\n",
    "            tmp['id'] = row['id']\n",
    "            sentences.append(tmp)\n",
    "        devProcessed[rId]['sentences'] = sentences\n",
    "    return devProcessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "devProcessed = loadDevFile(devDataFile,'intersentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "model = BertForNextSentencePrediction.from_pretrained(model_name)\n",
    "# Set the model in evaluation mode to deactivate the DropOut modules\n",
    "# This is IMPORTANT to have reproducible results during evaluation!\n",
    "model.eval()\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtainProb(tokenizer,model,seq_A,seq_B):\n",
    "    with torch.no_grad():\n",
    "        # encode the two sequences. Particularly, make clear that they must be \n",
    "        # encoded as \"one\" input to the model by using 'seq_B' as the 'text_pair'\n",
    "        encoded = tokenizer.encode_plus(seq_A, text_pair=seq_B, return_tensors='pt')\n",
    "        # NOTE when you print encoded you will see how the token_type_ids are 0 for all tokens in seq_A and 1 for seq_B, \n",
    "        # this way the model knows which token belongs to which sequence\n",
    "        # a model's output is a tuple, we only need the output tensor containing\n",
    "        # the relationships which is the first item in the tuple\n",
    "        seq_relationship_logits = model(**encoded)[0]\n",
    "        # we still need softmax to convert the logits into probabilities\n",
    "        # index 0: sequence B is a continuation of sequence A\n",
    "        # index 1: sequence B is a random sequence\n",
    "        probs = softmax(seq_relationship_logits, dim=1)\n",
    "        return probs[0][0].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the model probability for each of the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2123/2123 [03:27<00:00, 10.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for rid,row in tqdm(devProcessed.items()):\n",
    "    seq_A= row['context']\n",
    "    for inner_row in row['sentences']:\n",
    "        seq_B = inner_row['sentence']\n",
    "        gold_label= inner_row['gold_label']\n",
    "        inner_row['model_prob'] = obtainProb(tokenizer,model,seq_A,seq_B) # inplace update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to choose for cases where the stereotypes and non- stereo have a huge difference in probability for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createSmallerRecord(inner_row):\n",
    "    new_record = dict()\n",
    "    new_record['sentence'] = inner_row['sentence']\n",
    "    new_record['gold_label']=inner_row['gold_label']\n",
    "    new_record['model_prob']=inner_row['model_prob']\n",
    "    return new_record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a function for doing offlien analysis on records. It gives records that have a huge difference between stereo and non-stereo\n",
    "def CreateRecordsAnalysis(devProcessed,threshold=0.75,numDigits=3):\n",
    "    interesting_records =[]\n",
    "    for rid,row in devProcessed.items():\n",
    "        stereo_score=0\n",
    "        non_stereo_score =0\n",
    "        newSentences = []\n",
    "        for inner_row in row['sentences']:\n",
    "            gold_label= inner_row['gold_label']\n",
    "            score = round(inner_row['model_prob'] ,numDigits)\n",
    "\n",
    "            if gold_label =='stereotype':\n",
    "                stereo_score=score\n",
    "                newSentences.append(createSmallerRecord(inner_row))\n",
    "            elif gold_label=='anti-stereotype':\n",
    "                non_stereo_score=score\n",
    "                newSentences.append(createSmallerRecord(inner_row))\n",
    "        if abs(stereo_score-non_stereo_score) >threshold:\n",
    "            new_row = dict()\n",
    "            new_row['context'] = row['context']\n",
    "            new_row['bias_type'] =row['bias_type']\n",
    "            new_row['sentences'] = newSentences\n",
    "            interesting_records.append(new_row)\n",
    "    return interesting_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#writes records to separate files\n",
    "def writeData(outfile,interesting_records):\n",
    "    with open(outfile,'w',encoding='utf-8') as f:\n",
    "        for row in interesting_records:\n",
    "            json.dump(row,f)\n",
    "            f.write('\\n')\n",
    "    print(\"Wrote interesting records to \",outfile)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of interesting records  350\n",
      "Wrote interesting records to  analysisIntersentenceInteresting.json\n"
     ]
    }
   ],
   "source": [
    "# we are picking examples for visual analysis with this code\n",
    "interesting_records = CreateRecordsAnalysis(devProcessed,threshold=0.75)\n",
    "print(\"Number of interesting records \",len(interesting_records))\n",
    "writeData('analysisIntersentenceInteresting.json',interesting_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writing down interesting records\n",
    "with open('analysisIntersentenceInteresting.json','w',encoding='utf-8') as f:\n",
    "    for row in interesting_records:\n",
    "        json.dump(row,f)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to feed to StereoSet eval code, we need {'id': '107427644575c4712bf105f14475af0e', 'score': 0.058055270463228226} per sentence\n",
    "def CreateScoresListForPred(devProcessed):\n",
    "    SentenceScores = []\n",
    "    for index,row in devProcessed.items():\n",
    "        for inner_row in row['sentences']:\n",
    "            record =dict()\n",
    "            record['id'] = inner_row['id']\n",
    "            record['score'] = inner_row['model_prob']\n",
    "            SentenceScores.append(record)\n",
    "    print(\"Total Number of sentences added \", len(SentenceScores))\n",
    "    return SentenceScores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of sentences added  6369\n"
     ]
    }
   ],
   "source": [
    "InterSetenceList = CreateScoresListForPred(devProcessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intrasentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "devProcessed = loadDevFile(devDataFile,'intrasentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['cls.predictions.decoder.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "masked_LM_model = BertForMaskedLM.from_pretrained('bert-base-uncased').eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to process the input to add cls & sep token. Also we are replacing keyword BLANK with MASK\n",
    "def tokenizeText(text):\n",
    "    text = '[CLS]' + text + '[SEP]'\n",
    "    text = re.sub(r\"\\bBLANK\\b\",'[MASK]',text)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    return tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will generate all the required tensors\n",
    "def ProcessMaskedInput(text):\n",
    "    tokenized_text = tokenizeText(text)\n",
    "    masked_index=0\n",
    "    # we need masked_index to choose the output\n",
    "    for index,token in enumerate(tokenized_text):\n",
    "        if token=='[MASK]':\n",
    "            masked_index=index\n",
    "            break\n",
    "    # Convert token to vocabulary indices\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # Create the segments tensors.\n",
    "    segments_ids = [0] * len(tokenized_text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    return tokens_tensor, segments_tensors ,masked_index  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To know the prediction we need to know the position of the masked word\n",
    "def GetTokenIndex(word):\n",
    "    target = [word]\n",
    "    target_index= tokenizer.convert_tokens_to_ids(target)[0]\n",
    "    return target_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function runs the model for a text and list of candidates and returns \n",
    "def FindScoresForCand(text,cand_text_l):\n",
    "    # Predict all tokens\n",
    "    with torch.no_grad():\n",
    "        tokens_tensor, segments_tensors ,masked_index =ProcessMaskedInput(text)\n",
    "        predictions = masked_LM_model(tokens_tensor, segments_tensors)\n",
    "        # Transformers models always output tuples.In our case, the first element is the hidden state of the last layer \n",
    "        predictions_for_mask = predictions[0][0, masked_index] \n",
    "        probs = softmax(predictions_for_mask,dim=0)\n",
    "        cand_probs = []\n",
    "        for row in cand_text_l : \n",
    "            cand_text = row['sentence']\n",
    "            cand_tokenized_text = tokenizeText(cand_text)\n",
    "            word = cand_tokenized_text[masked_index]\n",
    "            row['model_prob']=probs[GetTokenIndex(word)].item() # in place replacement\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2106/2106 [01:39<00:00, 21.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# call the model and get the probability\n",
    "for rid,row in tqdm(devProcessed.items()):\n",
    "    seq_A= row['context']\n",
    "    candidates = row['sentences']\n",
    "    FindScoresForCand(seq_A,candidates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of interesting records  325\n",
      "Wrote interesting records to  analysisIntrasentenceInteresting.json\n"
     ]
    }
   ],
   "source": [
    "# this is to find interesting records for analysis\n",
    "interesting_records = CreateRecordsAnalysis(devProcessed,2.0e-02,6)\n",
    "print(\"Number of interesting records \",len(interesting_records))\n",
    "writeData('analysisIntrasentenceInteresting.json',interesting_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of sentences added  6318\n"
     ]
    }
   ],
   "source": [
    "IntraSetenceList = CreateScoresListForPred(devProcessed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Prepare output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "predData = dict()\n",
    "predData['intersentence'] = InterSetenceList\n",
    "predData['intrasentence'] = IntraSetenceList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write it to a file\n",
    "if not os.path.exists('gilopez_Predictions'):\n",
    "    os.makedirs('gilopez_Predictions')\n",
    "with open('gilopez_Predictions/predictedResults.json', 'w') as fp:\n",
    "    json.dump(predData, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it on Sample Data provided by Stereoset Authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating SamplePredictions/predictions_bert-base-cased_BertNextSentence_BertLM.json...\n",
      "intrasentence\n",
      "\tgender\n",
      "\t\tCount: 765.0\n",
      "\t\tLM Score: 82.50328729241772\n",
      "\t\tSS Score: 61.48204661682922\n",
      "\t\tICAT Score: 63.55715547775384\n",
      "\tprofession\n",
      "\t\tCount: 2430.0\n",
      "\t\tLM Score: 82.31092099986019\n",
      "\t\tSS Score: 60.8476591974996\n",
      "\t\tICAT Score: 64.45330461508425\n",
      "\trace\n",
      "\t\tCount: 2886.0\n",
      "\t\tLM Score: 83.82409779040428\n",
      "\t\tSS Score: 56.29627559199869\n",
      "\t\tICAT Score: 73.26850537162359\n",
      "\treligion\n",
      "\t\tCount: 237.0\n",
      "\t\tLM Score: 82.16091954022988\n",
      "\t\tSS Score: 56.27586206896552\n",
      "\t\tICAT Score: 71.84830757035274\n",
      "\toverall\n",
      "\t\tCount: 2106.0\n",
      "\t\tLM Score: 83.01912382272438\n",
      "\t\tSS Score: 58.68030062800166\n",
      "\t\tICAT Score: 68.60650476963355\n",
      "intersentence\n",
      "\tgender\n",
      "\t\tCount: 726.0\n",
      "\t\tLM Score: 90.84746774964165\n",
      "\t\tSS Score: 62.026618711401326\n",
      "\t\tICAT Score: 68.99571063921627\n",
      "\tprofession\n",
      "\t\tCount: 2481.0\n",
      "\t\tLM Score: 85.87218285497853\n",
      "\t\tSS Score: 62.32294703538678\n",
      "\t\tICAT Score: 64.70821563227955\n",
      "\trace\n",
      "\t\tCount: 2928.0\n",
      "\t\tLM Score: 89.67450156426334\n",
      "\t\tSS Score: 58.359268553512166\n",
      "\t\tICAT Score: 74.68223674470285\n",
      "\treligion\n",
      "\t\tCount: 234.0\n",
      "\t\tLM Score: 93.6455938697318\n",
      "\t\tSS Score: 61.03639846743295\n",
      "\t\tICAT Score: 72.97539209641667\n",
      "\toverall\n",
      "\t\tCount: 2123.0\n",
      "\t\tLM Score: 88.52986077301836\n",
      "\t\tSS Score: 60.43034761397915\n",
      "\t\tICAT Score: 70.06191633142319\n",
      "overall\n",
      "\tCount: 4229.0\n",
      "\tLM Score: 85.77180212754598\n",
      "\tSS Score: 59.555742005152865\n",
      "\tICAT Score: 69.37953787858896\n"
     ]
    }
   ],
   "source": [
    "%run -i evaluation.py --gold-file dev.json --predictions-dir SamplePredictions/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run it on the file we just created "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating gilopez_Predictions/predictedResults.json...\n",
      "intrasentence\n",
      "\tgender\n",
      "\t\tCount: 765.0\n",
      "\t\tLM Score: 85.26874960788005\n",
      "\t\tSS Score: 65.81614359875229\n",
      "\t\tICAT Score: 58.29629384219437\n",
      "\tprofession\n",
      "\t\tCount: 2430.0\n",
      "\t\tLM Score: 84.27860823500706\n",
      "\t\tSS Score: 59.80115458275169\n",
      "\t\tICAT Score: 67.75805488839758\n",
      "\trace\n",
      "\t\tCount: 2886.0\n",
      "\t\tLM Score: 87.00319891306857\n",
      "\t\tSS Score: 55.54717017291779\n",
      "\t\tICAT Score: 77.35076791388843\n",
      "\treligion\n",
      "\t\tCount: 237.0\n",
      "\t\tLM Score: 87.12643678160919\n",
      "\t\tSS Score: 58.94252873563219\n",
      "\t\tICAT Score: 71.54382349055356\n",
      "\toverall\n",
      "\t\tCount: 2106.0\n",
      "\t\tLM Score: 85.75367359929504\n",
      "\t\tSS Score: 58.591415011417865\n",
      "\t\tICAT Score: 71.01876562639082\n",
      "intersentence\n",
      "\tgender\n",
      "\t\tCount: 726.0\n",
      "\t\tLM Score: 89.45130151651892\n",
      "\t\tSS Score: 57.637827148696715\n",
      "\t\tICAT Score: 75.78702993233645\n",
      "\tprofession\n",
      "\t\tCount: 2481.0\n",
      "\t\tLM Score: 84.68382403929309\n",
      "\t\tSS Score: 62.345428969882065\n",
      "\t\tICAT Score: 63.7746613477914\n",
      "\trace\n",
      "\t\tCount: 2928.0\n",
      "\t\tLM Score: 87.77420257603487\n",
      "\t\tSS Score: 59.69733115785828\n",
      "\t\tICAT Score: 70.75069238609991\n",
      "\treligion\n",
      "\t\tCount: 234.0\n",
      "\t\tLM Score: 90.5919540229885\n",
      "\t\tSS Score: 62.13026819923371\n",
      "\t\tICAT Score: 68.6138600431585\n",
      "\toverall\n",
      "\t\tCount: 2123.0\n",
      "\t\tLM Score: 86.91993533101522\n",
      "\t\tSS Score: 60.534631226126926\n",
      "\t\tICAT Score: 68.6065460327943\n",
      "overall\n",
      "\tCount: 4229.0\n",
      "\tLM Score: 86.31169846142537\n",
      "\tSS Score: 59.590506619515416\n",
      "\tICAT Score: 69.756240152707\n"
     ]
    }
   ],
   "source": [
    "%run -i evaluation.py --gold-file dev.json --predictions-dir gilopez_Predictions/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "interpret_k1",
   "language": "python",
   "name": "interpret_k1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
